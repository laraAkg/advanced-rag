{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Exercise\n",
    "\n",
    "This notebook is designed as an exercise to build a complete Retrieval-Augmented Generation (RAG) system. In this exercise, you will integrate three main components into a single pipeline:\n",
    "\n",
    "1. **Retrieval Module** ‚Äì Retrieve relevant documents based on a query.\n",
    "2. **Transformation Module** ‚Äì Transform the retrieved queries.\n",
    "3. **Generation Module and Evaluation** ‚Äì Use the transformed data to generate responses and evaluate the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Building the RAG Pipeline\n",
    "\n",
    "Load the data and store it in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 61\n",
      "Beispiel-Chunk:\n",
      "High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 diabetes: A randomized \n",
      "trial\n",
      "Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3, Michael T. \n",
      "McDermott4, Victoria A. Catenacci4, Holly R. Wyatt1,5, James O. Hill1\n",
      "1Department of Nu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Der Code liest alle PDFs aus dem data-Ordner ein, extrahiert den Text seitenweise und f√ºgt alles zu einem gro√üen Textstring zusammen.\n",
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]\n",
    "### Split the data into chunks.\n",
    "# Text-Splitter konfigurieren\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Max. L√§nge pro Chunk\n",
    "    chunk_overlap=200       # √úberlappung zwischen Chunks\n",
    ")\n",
    "\n",
    "# Den extrahierten PDF-Text aufteilen\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Vorschau\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "print(f\"Beispiel-Chunk:\\n{chunks[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 61\n",
      "Preview of the first chunk: High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 diabetes: A randomized \n",
      "trial\n",
      "Julianne G. Clina1, R. Drew Sayer1,3, Zhaoxing Pan2, Caroline W. Cohen3,\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an embedding model\n",
    "Use the SentenceTransfomer wrapper as we have done so far.\n",
    "Models are found here: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "or on HuggingFace.\n",
    "\n",
    "Embed the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding-Shape: (61, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Der Code l√§dt ein vortrainiertes SentenceTransformer-Modell und wandelt alle Text-Chunks in numerische Vektor-Embeddings um, die sp√§ter f√ºr semantische Suche verwendet werden.\n",
    "# Modell ausw√§hlen ‚Äì du kannst auch ein anderes von sbert.net nehmen\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Chunks einbetten\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Form pr√ºfen\n",
    "print(f\"Embedding-Shape: {chunk_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Index and save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Wenn chunk_embeddings die Form (100, 384) hat, bedeutet das, dass es 100 \n",
    "# Embeddings gibt, und jedes Embedding hat 384 Dimensionen. In diesem Fall w√ºrde d = 384 sein.\n",
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FAISS-Index initialisieren\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Embeddings hinzuf√ºgen\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Key for language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a retriever function\n",
    "\n",
    "arguments: query, k, index, chunks, embedding model\n",
    "\n",
    "return: retrieved texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "platform partners. Journal of biomedical informatics, 2019. 95: p. 103208.\n",
      "34. Harris PA, et al. , Research electronic data capture (REDCap)‚Äîa metadata-driven methodology \n",
      "and workflow process for providing translational research informatics support. Journal of \n",
      "biomedical informatics, 2009. 42(2): p. 377‚Äì381. [PubMed: 18929686] \n",
      "35. Wing RR, et al. , Benefits of Modest Weight Loss in Improving Cardiovascular Risk Factors in \n",
      "Overweight and Obese Individuals With Type 2 Diabetes. Diabetes Care, 2011. 34(7): p. 1481‚Äì\n",
      "1486. [PubMed: 21593294] \n",
      "36. Lau DCW and Teoh H, Benefits of Modest Weight Loss on the Management of Type 2 Diabetes \n",
      "Mellitus. Canadian Journal of Diabetes, 2013. 37(2): p. 128‚Äì134. [PubMed: 24070804] \n",
      "37. Huang S, et al. , Association of magnitude of weight loss and weight variability with mortality and \n",
      "major cardiovascular events among individuals with type 2 diabetes mellitus: a systematic review\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query, k, index, chunks, embedding_model):\n",
    "    # 1. Embed die Abfrage\n",
    "    print(type(query))\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # 2. Suche im FAISS-Index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # 3. Hole die zugeh√∂rigen Text-Chunks\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "\n",
    "    return retrieved_texts, distances[0]\n",
    "\n",
    "results, scores = retrieve(\"What is deep learning?\", 3, index, chunks, model)\n",
    "print(results[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build an answer function\n",
    "Build an answer function that takes a query, k, an index and the chunks.\n",
    "\n",
    "return: answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def answer_query(query, k, index, chunks, embedding_model):\n",
    "    load_dotenv()\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    retrieved_texts, _ = retrieve(query, k, index, chunks, embedding_model)\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the following question based only on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** retrieve()\tSuchen\tHol die relevantesten Textst√ºcke zu einer Frage.\n",
    "\n",
    "**answer_query()\tAntworten\tBau ein Prompt aus den gefundenen Textst√ºcken und frag GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LLM Answer: The most important factor in diagnosing asthma is typically a combination of symptoms, medical history, and lung function tests.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query(query, 5, index, chunks, model)\n",
    "print(\"LLM Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Rewriter\n",
    "\n",
    "Take a query and an api key for the model and rewrite the query. \n",
    "\n",
    "Rewriting a query: A Language Model is prompted to rewrite a query to better suit a task.\n",
    "\n",
    "Other Transfomrations are implemented in a similar fashion, this is just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def rewrite_query(original_query):\n",
    "    load_dotenv()\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Rewrite the following question to be more specific and better suited for information retrieval:\n",
    "\n",
    "Original:\n",
    "{original_query}\n",
    "\n",
    "Improved:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: What physiological processes characterize asthma and contribute to its symptoms?\n"
     ]
    }
   ],
   "source": [
    "query = \"How does asthma work?\"\n",
    "improved_query = rewrite_query(query)\n",
    "print(\"Rewritten Query:\", improved_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the rewriter into your answer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\t‚úèÔ∏è Die urspr√ºngliche Frage wird umformuliert (pr√§ziser, besser f√ºr die Suche gemacht).\n",
    "\n",
    "2\tüîç Mit der verbesserten Frage wird nach den relevantesten Chunks gesucht und dann eine Antwort erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def rewrite_query_groq(query, groq_api_key):\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Rewrite the following query to be more precise and optimized for retrieval:\n",
    "\n",
    "Original:\n",
    "{query}\n",
    "\n",
    "Improved:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",  # oder ein anderes aktives Modell\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def answer_query_with_rewriting(query, k, index, chunks, groq_api_key):\n",
    "    # ‚úèÔ∏è Rewrite the input query first\n",
    "    improved_query = rewrite_query_groq(query, groq_api_key)\n",
    "    print(type(improved_query))\n",
    "\n",
    "    print(\"üîÅ Rewritten Query:\", improved_query)\n",
    "\n",
    "    # üîç Retrieve relevant chunks\n",
    "    embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    retrieved_texts, _ = retrieve(improved_query, k, index, chunks, embedding_model)\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    # üß† Ask the Groq LLM\n",
    "    prompt = f\"\"\"\n",
    "Answer the following question based only on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{improved_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "üîÅ Rewritten Query: Here's an improved query:\n",
      "\n",
      "**What are the top 2-3 clinically validated factors that are consistently identified as crucial in diagnosing asthma, as supported by peer-reviewed research?**\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. **Precision**: The original query is too broad and open-ended, making it difficult to provide a clear answer. The improved query narrows down the scope to focus on the top factors, which helps to provide more precise and actionable information.\n",
      "2. **Optimization for retrieval**: The improved query is structured to retrieve specific, clinically validated information from peer-reviewed research, which is more likely to provide accurate and reliable answers.\n",
      "3. **Relevance**: By specifying \"clinically validated factors\" and \"peer-reviewed research\", the improved query ensures that the retrieved results are relevant to the diagnosis of asthma and grounded in scientific evidence.\n",
      "4. **Avoiding ambiguity**: The improved query clarifies that the factors should be \"crucial\" and \"consistently identified\", which helps to rule out less important or less well-established factors.\n",
      "\n",
      "By asking a more precise and optimized question, the improved query is more likely to retrieve relevant and accurate information that can inform healthcare professionals in their diagnostic decisions.\n",
      "<class 'str'>\n",
      "LLM Answer: Based on the provided context, there is no information about asthma or the diagnosis of asthma. The context discusses a clinical trial investigating the effects of high-protein versus normal-protein diets on patients with type 2 diabetes. Therefore, it is not possible to answer the question about diagnosing asthma.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_api_key)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 .Evaluation\n",
    "\n",
    "Select random chunks from all your chunks, and generate a question to each of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W√§hlt zuf√§llig Text-Chunks aus und l√§sst GPT f√ºr jeden Chunk eine passende Frage generieren, \n",
    "# mit automatischen Retry-Versuchen bei Timeout-Fehlern.\n",
    "\n",
    "import time\n",
    "import httpx  # Ensure you're catching the correct timeout exception\n",
    "from openai import OpenAI\n",
    "def generate_questions_for_random_chunks(chunks, num_chunks=20, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 20).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Randomly select the desired number of chunks.\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client once\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm.tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk.\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with simple retry logic.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                     model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait a bit before retrying.\n",
    "        \n",
    "        # If all attempts fail, use an error message as the generated question.\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Study [ 25] to detect a 2.75 kg difference in weight loss between the HP and NP. Weight \n",
      "loss achiev...\n",
      "Generated Question: What statistical methods were employed in the study to assess differences in weight loss between the HP and NP diet groups, and what factors were considered in the randomization process?\n",
      "\n",
      "Chunk 2:\n",
      "weight improvement, satisfaction and energy. Obesity science & practice, 2017. 3(3): p. 298‚Äì310. \n",
      "[P...\n",
      "Generated Question: What recent research findings indicate about the impact of red meat consumption on weight management, vascular health, and risk factors for conditions like type 2 diabetes?\n",
      "\n",
      "Chunk 3:\n",
      "preferential loss of fat mass compared to fat free mass which was also not supported. A \n",
      "recent revi...\n",
      "Generated Question: What implications do the findings on body composition changes during weight loss in individuals with Type 2 Diabetes have for dietary recommendations, particularly regarding the effects of high protein versus high carbohydrate diets?\n",
      "\n",
      "Chunk 4:\n",
      "High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 di...\n",
      "Generated Question: What impact do high protein diets, particularly those including red meat, have on weight loss, body composition, and glucose control in adults with type 2 diabetes?\n",
      "\n",
      "Chunk 5:\n",
      "Participants received copies of the SOS book, copies of the course materials, and access \n",
      "to the onl...\n",
      "Generated Question: What were the key components of the SOS intervention program, and how were participants differentiated in terms of dietary guidelines?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=5, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Test the questions with your built retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    For each (chunk, generated_question) tuple in the provided list, use the prebuilt\n",
    "    retrieval function to generate an answer for the generated question. The function\n",
    "    returns a list of dictionaries containing the original chunk, the generated question,\n",
    "    and the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - question_tuples (list of tuples): Each tuple is (chunk, generated_question)\n",
    "    - k (int): Number of retrieved documents to use for answering.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dict): Each dict contains 'chunk', 'question', and 'answer'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        # Use your retrieval-based answer function. Here we assume the function signature is:\n",
    "        # answer_query(query, k, index, texts, groq_api_key)\n",
    "        embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        answer = answer_query(question, k, index, texts, embedding_model=embedding_model) #query, k, index,texts\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "Chunk Preview: Study [ 25] to detect a 2.75 kg difference in weight loss between the HP and NP. Weight \n",
      "loss achiev\n",
      "Generated Question: What statistical methods were employed in the study to assess differences in weight loss between the HP and NP diet groups, and what factors were considered in the randomization process?\n",
      "Answer: Linear mixed models (LMM) with unstructured covariance were used to test the effect of diet group, time, and their interaction between HP and NP. \n",
      "Randomization was performed by the statistician and was stratified by age, sex, BMI, and years since diagnosis of T2D.\n",
      "-----------------------------\n",
      "Chunk Preview: weight improvement, satisfaction and energy. Obesity science & practice, 2017. 3(3): p. 298‚Äì310. \n",
      "[P\n",
      "Generated Question: What recent research findings indicate about the impact of red meat consumption on weight management, vascular health, and risk factors for conditions like type 2 diabetes?\n",
      "Answer: Recent research findings suggest that red meat consumption may not have a significant impact on weight management, vascular health, or risk factors for conditions like type 2 diabetes. Studies have shown that both high protein diets containing red meat and normal protein diets without red meat were effective at producing weight loss and improvements in glucose control. Additionally, there were no observed effects of dietary protein and red meat consumption on weight loss and improved cardiometabolic health; suggesting that achieved weight loss, rather than diet composition, should be the primary focus of dietary interventions for managing type 2 diabetes.\n",
      "-----------------------------\n",
      "Chunk Preview: preferential loss of fat mass compared to fat free mass which was also not supported. A \n",
      "recent revi\n",
      "Generated Question: What implications do the findings on body composition changes during weight loss in individuals with Type 2 Diabetes have for dietary recommendations, particularly regarding the effects of high protein versus high carbohydrate diets?\n",
      "Answer: The findings suggest that a high protein diet, specifically one including lean beef, did not result in greater weight loss or preferential loss of fat mass compared to fat free mass in individuals with Type 2 Diabetes. This implies that the type of protein consumed during weight loss may not have a significant impact on body composition changes in this population. Therefore, dietary recommendations for individuals with Type 2 Diabetes may not need to prioritize high protein diets over high carbohydrate diets for weight loss and body composition changes.\n",
      "-----------------------------\n",
      "Chunk Preview: High and normal protein diets improve body composition and \n",
      "glucose control in adults with type 2 di\n",
      "Generated Question: What impact do high protein diets, particularly those including red meat, have on weight loss, body composition, and glucose control in adults with type 2 diabetes?\n",
      "Answer: High protein diets, including those that include red meat, have been shown to be effective at promoting weight loss, improving body composition, and enhancing glucose control in adults with type 2 diabetes.\n",
      "-----------------------------\n",
      "Chunk Preview: Participants received copies of the SOS book, copies of the course materials, and access \n",
      "to the onl\n",
      "Generated Question: What were the key components of the SOS intervention program, and how were participants differentiated in terms of dietary guidelines?\n",
      "Answer: The key components of the SOS intervention program included the SOS book, course materials, and access to an online community. Participants also participated in 18 bi-weekly group classes in the SOS Next Steps program. Participants were randomly assigned to one of two diet groups: the high protein group (HP) was instructed to consume lean beef as the only source of red meat, while the normal protein group (NP) was instructed to not eat red meat and follow a modified SOS diet with reduced protein intake.\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "results = answer_generated_questions(questions, 5, index, chunks, groq_api_key)\n",
    "\n",
    "for item in results:\n",
    "    print(\"Chunk Preview:\", item['chunk'][:100])\n",
    "    print(\"Generated Question:\", item['question'])\n",
    "    print(\"Answer:\", item['answer'])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_answers_binary(results, groq_api_key, max_retries=3):\n",
    "    \"\"\"\n",
    "    Evaluates each answer in the results list using an LLM.\n",
    "    For each result (a dictionary containing 'chunk', 'question', and 'answer'),\n",
    "    it sends an evaluation prompt to the Groq LLM which outputs 1 if the answer is on point,\n",
    "    and 0 if it is missing the point.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list of dict): Each dict must contain keys 'chunk', 'question', and 'answer'.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - max_retries (int): Maximum number of retries if the API call times out.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A dataframe containing the original chunk, question, answer, and evaluation score.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    for item in tqdm.tqdm(results, desc=\"Evaluating Answers\"):\n",
    "        # Build the evaluation prompt.\n",
    "        prompt = (\n",
    "            \"Evaluate the following answer to the given question. \"\n",
    "            \"If the answer is accurate and complete, reply with 1. \"\n",
    "            \"If the answer is inaccurate, incomplete, or otherwise not acceptable, reply with 0. \"\n",
    "            \"Do not include any extra text.\\n\\n\"\n",
    "            \"Question: \" + item['question'] + \"\\n\\n\"\n",
    "            \"Answer: \" + item['answer'] + \"\\n\\n\"\n",
    "            \"Context (original chunk): \" + item['chunk'] + \"\\n\\n\"\n",
    "            \"Evaluation (1 for good, 0 for bad):\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        generated_eval = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Retry logic in case of timeouts or errors.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"4o-mini\"\n",
    "                )\n",
    "                generated_eval = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the retry loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred during evaluation. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error during evaluation: {e}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If no valid evaluation was produced, default to 0.\n",
    "        if generated_eval is None:\n",
    "            generated_eval = \"0\"\n",
    "        \n",
    "        # Convert the response to an integer (1 or 0).\n",
    "        try:\n",
    "            score = int(generated_eval)\n",
    "            if score not in [0, 1]:\n",
    "                score = 0\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        evaluations.append(score)\n",
    "    \n",
    "    # Add the evaluation score to each result.\n",
    "    for i, item in enumerate(results):\n",
    "        item['evaluation'] = evaluations[i]\n",
    "    \n",
    "    # Create a dataframe for manual review.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  20%|‚ñà‚ñà        | 1/5 [00:06<00:25,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:12<00:19,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:19<00:12,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:25<00:06,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:31<00:00,  6.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Study [ 25] to detect a 2.75 kg difference in ...</td>\n",
       "      <td>What statistical methods were employed in the ...</td>\n",
       "      <td>Linear mixed models (LMM) with unstructured co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weight improvement, satisfaction and energy. O...</td>\n",
       "      <td>What recent research findings indicate about t...</td>\n",
       "      <td>Recent research findings suggest that red meat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferential loss of fat mass compared to fat ...</td>\n",
       "      <td>What implications do the findings on body comp...</td>\n",
       "      <td>The findings suggest that a high protein diet,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High and normal protein diets improve body com...</td>\n",
       "      <td>What impact do high protein diets, particularl...</td>\n",
       "      <td>High protein diets, including those that inclu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Participants received copies of the SOS book, ...</td>\n",
       "      <td>What were the key components of the SOS interv...</td>\n",
       "      <td>The key components of the SOS intervention pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  Study [ 25] to detect a 2.75 kg difference in ...   \n",
       "1  weight improvement, satisfaction and energy. O...   \n",
       "2  preferential loss of fat mass compared to fat ...   \n",
       "3  High and normal protein diets improve body com...   \n",
       "4  Participants received copies of the SOS book, ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What statistical methods were employed in the ...   \n",
       "1  What recent research findings indicate about t...   \n",
       "2  What implications do the findings on body comp...   \n",
       "3  What impact do high protein diets, particularl...   \n",
       "4  What were the key components of the SOS interv...   \n",
       "\n",
       "                                              answer  evaluation  \n",
       "0  Linear mixed models (LMM) with unstructured co...           0  \n",
       "1  Recent research findings suggest that red meat...           0  \n",
       "2  The findings suggest that a high protein diet,...           0  \n",
       "3  High protein diets, including those that inclu...           0  \n",
       "4  The key components of the SOS intervention pro...           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_evaluations = evaluate_answers_binary(results, openai_api_key)\n",
    "display(df_evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
